# üöÄ Stack IA Ultime - Version Compl√®te Optimis√©e

Stack d'intelligence artificielle **ultra-compl√®te** et **optimis√©e** avec cache, load balancing, analytics, et agent autonome.

## ‚ú® Ce Que Vous Avez

### üéØ Services IA

| Service | Port | Description | Type |
|---------|------|-------------|------|
| **ü¶Ü Goose** | CLI | Agent autonome qui **fait** les choses | Agent |
| **‚ö° LiteLLM** | 4000 | Gateway intelligent + cache + load balancing | Gateway |
| **üìä Langfuse** | 3002 | Analytics et observabilit√© IA | Analytics |
| **üé® Open WebUI** | 3000 | Interface chat moderne (type ChatGPT) | Interface |
| **üîç Perplexica** | 3001 | Recherche web avec IA (type Perplexity) | Recherche |
| **üåê AI Gateway** | 8000 | API unifi√©e multi-providers | API |
| **ü§ñ Tabby** | 8080 | Autocompl√©tion de code | IDE |
| **üß† Ollama** | 11434 | LLM local priv√© | Mod√®le |
| **üíæ Redis** | 6379 | Cache ultra-rapide | Cache |

### üéÅ Providers IA Gratuits

- ‚úÖ **Groq** - Ultra-rapide (14,400 req/jour)
- ‚úÖ **Ollama** - Local et priv√© (illimit√©)
- ‚úÖ **Hugging Face** - Gratuit
- ‚úÖ **Together AI** - $25 cr√©dits gratuits
- ‚úÖ **OpenRouter** - Mod√®les gratuits

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  INTERFACES UTILISATEUR                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ Open    ‚îÇ  ‚îÇPerplexica ‚îÇ  ‚îÇ   Goose    ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ WebUI   ‚îÇ  ‚îÇ (Search)  ‚îÇ  ‚îÇ   (CLI)    ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ       ‚îÇ             ‚îÇ                ‚îÇ                   ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                     ‚îÇ                                    ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ       ‚îÇ  LiteLLM Proxy + Cache     ‚îÇ                    ‚îÇ
‚îÇ       ‚îÇ  ‚Ä¢ Load Balancing          ‚îÇ                    ‚îÇ
‚îÇ       ‚îÇ  ‚Ä¢ Redis Cache (x10)       ‚îÇ                    ‚îÇ
‚îÇ       ‚îÇ  ‚Ä¢ Auto Retry/Fallback     ‚îÇ                    ‚îÇ
‚îÇ       ‚îÇ  ‚Ä¢ Rate Limiting           ‚îÇ                    ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                     ‚îÇ                                    ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ       ‚îÇ     AI Gateway             ‚îÇ                    ‚îÇ
‚îÇ       ‚îÇ  Route vers providers      ‚îÇ                    ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ          ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ     ‚îÇ                         ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ    ‚îÇGroq  ‚îÇ ‚îÇOlla‚îÇ ‚îÇHF  ‚îÇ ‚îÇToge‚îÇ ‚îÇOpen ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ(‚ö°)  ‚îÇ ‚îÇma  ‚îÇ ‚îÇ(üÜì)‚îÇ ‚îÇther‚îÇ ‚îÇRoute‚îÇ            ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ       ‚îÇ  Langfuse (Analytics)       ‚îÇ                  ‚îÇ
‚îÇ       ‚îÇ  Track tout en temps r√©el   ‚îÇ                  ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Installation (10 Minutes)

### √âtape 1 : Pr√©requis

```bash
# V√©rifier Docker
docker --version  # >= 20.10
docker compose version  # >= 2.0

# RAM recommand√©e
# Minimum : 8 GB
# Standard : 16 GB
# Optimal : 32 GB
```

### √âtape 2 : Configuration

```bash
cd AI-Ultimate-Stack/

# Cr√©er .env
cp .env.example .env

# √âditer (OBLIGATOIRE)
nano .env
```

**Configuration minimale** :
```bash
# 1. Obtenir une cl√© Groq (2 min)
# ‚Üí https://console.groq.com/
GROQ_API_KEY=gsk_votre_cle_ici

# 2. Changer les secrets
LITELLM_MASTER_KEY=sk-votre-secret-unique
LANGFUSE_NEXTAUTH_SECRET=$(openssl rand -base64 32)
LANGFUSE_SALT=$(openssl rand -base64 32)
WEBUI_SECRET_KEY=$(openssl rand -base64 32)
```

### √âtape 3 : Lancer

```bash
# D√©marrer tout
docker compose up -d

# Suivre les logs
docker compose logs -f

# Attendre que tout soit pr√™t (2-5 min)
# V√©rifier : docker compose ps
```

### √âtape 4 : V√©rifier

```bash
# Health checks
curl http://localhost:4000/health  # LiteLLM
curl http://localhost:8000/health  # AI Gateway
curl http://localhost:3002/api/public/health  # Langfuse

# Ou utiliser le script de validation
./scripts/validate-stack.sh
```

---

## üí° Utilisation

### üé® Open WebUI (Interface Chat)

```bash
# Ouvrir dans le navigateur
open http://localhost:3000

# Premier acc√®s :
# 1. Cr√©er un compte admin
# 2. S√©lectionner un mod√®le
# 3. Commencer √† chatter !
```

**Fonctionnalit√©s** :
- Chat avec n'importe quel mod√®le
- Upload de documents (RAG)
- Recherche web int√©gr√©e
- Historique des conversations
- Partage de chats

### ü¶Ü Goose (Agent Autonome)

```bash
# Session interactive
docker compose exec goose goose session

# Commande directe
docker compose exec goose goose "Create a Python FastAPI hello world"

# Multi-√©tapes
docker compose exec goose goose "
  1. Create a new Python project
  2. Add FastAPI and pytest
  3. Write a /users endpoint
  4. Write tests
  5. Create a Dockerfile
"
```

**Goose peut** :
- ‚úÖ Ex√©cuter des commandes
- ‚úÖ Modifier des fichiers
- ‚úÖ Utiliser Git
- ‚úÖ Lancer Docker
- ‚úÖ Naviguer dans votre code
- ‚úÖ **Agir de mani√®re autonome**

### üîç Perplexica (Recherche Web IA)

```bash
# Ouvrir dans le navigateur
open http://localhost:3001

# Exemple de recherche :
# "What are the latest developments in AI agents?"
```

**Perplexica** :
1. Recherche sur le web
2. Analyse les r√©sultats avec l'IA
3. G√©n√®re une r√©ponse synth√©tique avec sources

### ‚ö° LiteLLM (Gateway Intelligent)

```bash
# API compatible OpenAI
curl http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer sk-1234" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "groq/llama-3.3-70b-versatile",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# UI Admin
open http://localhost:4000/ui
```

**LiteLLM fait** :
- ‚úÖ Cache les r√©ponses (x10 vitesse)
- ‚úÖ Load balancing automatique
- ‚úÖ Retry si √©chec
- ‚úÖ Fallback sur autre provider
- ‚úÖ Rate limiting

### üìä Langfuse (Analytics)

```bash
# Ouvrir le dashboard
open http://localhost:3002

# Premi√®re connexion :
# 1. Cr√©er un compte
# 2. Cr√©er un projet
# 3. R√©cup√©rer les cl√©s API
# 4. Les ajouter dans .env
```

**Langfuse montre** :
- Toutes les requ√™tes IA
- Latences et performances
- Co√ªts (m√™me si gratuit)
- Erreurs et debug
- Conversations compl√®tes

---

## üéØ Cas d'Usage Complets

### 1. D√©veloppement Assist√© par IA

**Setup** :
```bash
# Dans VSCode
# 1. Installer extension "Tabby"
#    ‚Üí Endpoint: http://localhost:8080
#
# 2. Installer extension "Continue"
#    ‚Üí Base URL: http://localhost:4000/v1
#    ‚Üí API Key: sk-1234
```

**Workflow** :
1. **Taper du code** ‚Üí Tabby autocomplete ‚ú®
2. **Besoin d'aide** ‚Üí Continue.dev (Cmd/Ctrl+L)
3. **T√¢che complexe** ‚Üí Goose agent
4. **Recherche web** ‚Üí Perplexica

### 2. Automatisation Compl√®te avec Goose

```bash
# Goose comprend et ex√©cute tout
docker compose exec goose goose "
  I need a new microservice:
  1. Create a Python FastAPI project
  2. Add endpoints for CRUD operations on users
  3. Use SQLAlchemy with PostgreSQL
  4. Add input validation with Pydantic
  5. Write comprehensive tests
  6. Create a Dockerfile with multi-stage build
  7. Add docker-compose.yml for dev
  8. Write README with API documentation
  9. Initialize git and make initial commit
"
```

Goose va **tout faire automatiquement** ! ü§Ø

### 3. Review de Code avec IA + Analytics

```bash
# Dans votre pipeline Woodpecker
steps:
  ai-review:
    image: python:3.11-slim
    commands:
      - pip install openai
      - |
        python << 'EOF'
        import openai
        
        client = openai.OpenAI(
            base_url="http://litellm:4000/v1",
            api_key="sk-1234"
        )
        
        response = client.chat.completions.create(
            model="groq/llama-3.3-70b-versatile",
            messages=[{
                "role": "user",
                "content": "Review this PR and suggest improvements"
            }]
        )
        
        print(response.choices[0].message.content)
        EOF
```

**R√©sultat** :
- Review instantan√©e (Groq < 1s)
- Track√©e dans Langfuse
- Cach√©e dans Redis
- Si Groq down ‚Üí fallback Ollama

### 4. Recherche + G√©n√©ration

```bash
# Rechercher des infos r√©centes
# http://localhost:3001
# "What are the new features in Python 3.13?"

# Utiliser ces infos dans Open WebUI
# http://localhost:3000
# "Based on Python 3.13 features, write a modern async app"
```

---

## ‚ö° Optimisations Incluses

### 1. Cache Redis (x10 Vitesse)

```python
# Requ√™te 1 : 2 secondes (appel API)
response = client.chat("Explain async/await")

# Requ√™te 2 (identique) : 0.02 seconde ! (cache)
response = client.chat("Explain async/await")
```

**Gain** : 100x plus rapide pour requ√™tes identiques

### 2. Load Balancing Automatique

```
Request 1 ‚Üí Groq (rapide, disponible)
Request 2 ‚Üí Groq (rate limit) ‚Üí Together AI
Request 3 ‚Üí Together AI (down) ‚Üí Ollama
```

**R√©sultat** : Toujours une r√©ponse, jamais de downtime

### 3. Retry Automatique

```python
# Plus besoin de g√©rer les erreurs
try:
    response = client.chat(...)  
    # LiteLLM retry automatiquement 3x
except:
    # Seulement si TOUS les providers ont √©chou√©
    pass
```

### 4. Analytics Temps R√©el

```
Langfuse Dashboard :
‚îú‚îÄ Requ√™tes : 1,245 aujourd'hui
‚îú‚îÄ Latence moyenne : 0.8s
‚îú‚îÄ Provider le plus utilis√© : Groq (95%)
‚îú‚îÄ Co√ªt : $0 (gratuit!)
‚îú‚îÄ Taux d'erreur : 0.1%
‚îî‚îÄ Cache hit rate : 45%
```

---

## üìä Comparaison de Performance

### Sans Optimisations (Stack Basique)

```
Requ√™te ‚Üí Groq API ‚Üí 1-2s
Si rate limit ‚Üí ‚ùå Erreur
Si down ‚Üí ‚ùå Erreur
M√™me requ√™te ‚Üí 1-2s (re-calcul)
```

### Avec Optimisations (Cette Stack)

```
Requ√™te 1 ‚Üí LiteLLM ‚Üí Groq ‚Üí 0.8s
  ‚îî‚îÄ Mise en cache Redis

Requ√™te 2 (identique) ‚Üí Redis ‚Üí 0.02s ‚ö°
  ‚îî‚îÄ 40x plus rapide !

Requ√™te 3 (rate limit) :
  LiteLLM ‚Üí Groq ‚ùå ‚Üí Together AI ‚úÖ ‚Üí 1.2s
  ‚îî‚îÄ Fallback automatique

Requ√™te 4 (all providers down) :
  LiteLLM ‚Üí Groq ‚ùå ‚Üí Together ‚ùå ‚Üí Ollama ‚úÖ ‚Üí 3s
  ‚îî‚îÄ Toujours une r√©ponse !
```

**R√©sultat** :
- ‚úÖ 40x plus rapide (cache)
- ‚úÖ 99.9% uptime (fallback)
- ‚úÖ Transparent pour l'utilisateur

---

## üîß Configuration Avanc√©e

### Profils de Performance

#### Minimal (8 GB RAM)
```bash
OLLAMA_MODELS=llama3.2,nomic-embed-text
TABBY_MODEL=StarCoder-1B
OLLAMA_NUM_CTX=2048
```

#### Standard (16 GB RAM)
```bash
OLLAMA_MODELS=codellama,llama3.2,nomic-embed-text
TABBY_MODEL=StarCoder-3B
OLLAMA_NUM_CTX=4096
GROQ_API_KEY=<votre_cl√©>
```

#### Performance (32 GB RAM + GPU)
```bash
OLLAMA_MODELS=deepseek-coder,qwen2.5-coder,nomic-embed-text
TABBY_MODEL=DeepSeek-Coder-6.7B
TABBY_DEVICE=cuda
OLLAMA_NUM_GPU=999
GROQ_API_KEY=<votre_cl√©>
TOGETHER_API_KEY=<votre_cl√©>
```

### Activer Perplexica (Recherche Web)

```bash
# D√©marrer avec Perplexica
docker compose --profile perplexica up -d

# Acc√©der
open http://localhost:3001
```

---

## üêõ Troubleshooting

### LiteLLM ne d√©marre pas

```bash
# V√©rifier les logs
docker compose logs litellm

# Probl√®me courant : config.yaml invalide
# V√©rifier la syntaxe YAML
```

### Cache Redis ne fonctionne pas

```bash
# V√©rifier Redis
docker compose exec redis redis-cli ping
# Doit retourner : PONG

# V√©rifier la connexion
docker compose logs litellm | grep redis
```

### Goose ne trouve pas les mod√®les

```bash
# V√©rifier que LiteLLM est pr√™t
curl http://localhost:4000/v1/models

# V√©rifier la config Goose
docker compose exec goose cat /root/.config/goose/config.yaml
```

### Langfuse n'enregistre pas

```bash
# V√©rifier les cl√©s dans .env
# LANGFUSE_PUBLIC_KEY et LANGFUSE_SECRET_KEY

# Les obtenir depuis Langfuse UI
open http://localhost:3002
# Settings ‚Üí API Keys
```

---

## üîí S√©curit√© Production

### Checklist

- [ ] Changer **TOUS** les secrets par d√©faut
- [ ] Activer HTTPS (reverse proxy)
- [ ] Limiter acc√®s r√©seau (localhost uniquement)
- [ ] Activer authentification partout
- [ ] Backups automatiques des volumes
- [ ] Monitoring (Langfuse + logs)
- [ ] Rate limiting strict
- [ ] Ne PAS exposer ports internes

### G√©n√©rer des Secrets Forts

```bash
# Master key LiteLLM
openssl rand -base64 48

# Secrets Langfuse
openssl rand -base64 32

# Secret WebUI
openssl rand -base64 32
```

---

## üìà Monitoring

### Dashboard Langfuse

```
URL : http://localhost:3002

M√©triques :
‚îú‚îÄ Requ√™tes totales
‚îú‚îÄ Latence P50/P95/P99
‚îú‚îÄ Co√ªts par provider
‚îú‚îÄ Taux d'erreur
‚îú‚îÄ Distribution des mod√®les
‚îî‚îÄ Conversations compl√®tes
```

### Logs Centralis√©s

```bash
# Tous les services
docker compose logs -f

# Service sp√©cifique
docker compose logs -f litellm
docker compose logs -f ai-gateway
docker compose logs -f goose
```

---

## üéÅ Bonus : Int√©gration Forgejo

### Connecter avec la Stack Forgejo

```bash
# Cr√©er r√©seau partag√©
docker network create dev-bridge

# Connecter Forgejo
docker network connect dev-bridge forgejo
docker network connect dev-bridge woodpecker-server

# Connecter IA
docker network connect dev-bridge litellm
docker network connect dev-bridge ai-gateway
docker network connect dev-bridge goose
```

### Pipeline Woodpecker avec IA

```yaml
# .woodpecker.yml
steps:
  ai-review:
    image: python:3.11-slim
    commands:
      - pip install openai
      - |
        python << 'EOF'
        import openai
        client = openai.OpenAI(
            base_url="http://litellm:4000/v1",
            api_key="sk-1234"
        )
        # Review de code avec cache + fallback
        response = client.chat.completions.create(...)
        EOF
```

---

## ‚úÖ Checklist Finale

- [ ] Docker + Docker Compose install√©s
- [ ] Cl√© Groq obtenue (gratuit)
- [ ] `.env` configur√©
- [ ] Secrets chang√©s
- [ ] Stack d√©marr√©e : `docker compose up -d`
- [ ] Services healthy : `docker compose ps`
- [ ] Open WebUI accessible : http://localhost:3000
- [ ] Langfuse configur√© : http://localhost:3002
- [ ] Goose test√© : `docker compose exec goose goose session`
- [ ] Cache Redis actif
- [ ] Int√©gr√© dans IDE (Tabby + Continue.dev)

---

## üéâ Vous Avez Maintenant

‚úÖ **9 Services IA** ultra-optimis√©s  
‚úÖ **Cache Redis** (x10-100 vitesse)  
‚úÖ **Load Balancing** automatique  
‚úÖ **Fallback** multi-providers  
‚úÖ **Analytics** temps r√©el  
‚úÖ **Agent Autonome** (Goose)  
‚úÖ **Interface Moderne** (Open WebUI)  
‚úÖ **Recherche Web IA** (Perplexica)  
‚úÖ **100% Auto-h√©berg√©** et gratuit  

**La stack IA la plus compl√®te et optimis√©e du march√© ! üöÄ**

---

<div align="center">

**Fait avec ‚ù§Ô∏è pour les d√©veloppeurs**

*Stack optimis√©e avec 15 ans d'expertise*

</div>
