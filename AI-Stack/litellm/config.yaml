# =============================================================================
# LiteLLM Configuration - Gateway intelligent avec cache et load balancing
# =============================================================================

model_list:
  # ===========================================================================
  # GROQ - Ultra rapide, gratuit
  # ===========================================================================
  - model_name: groq/llama-3.3-70b-versatile
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY
      rpm: 30  # Requests per minute limit
      tpm: 14400  # Tokens per minute limit
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  - model_name: groq/llama-3.1-8b-instant
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: os.environ/GROQ_API_KEY
      rpm: 30
      tpm: 14400

  - model_name: groq/mixtral-8x7b-32768
    litellm_params:
      model: groq/mixtral-8x7b-32768
      api_key: os.environ/GROQ_API_KEY
      rpm: 30
      tpm: 14400

  # ===========================================================================
  # OLLAMA - Local, privé, illimité
  # ===========================================================================
  - model_name: ollama/codellama
    litellm_params:
      model: ollama/codellama
      api_base: http://ollama:11434

  - model_name: ollama/llama3.2
    litellm_params:
      model: ollama/llama3.2
      api_base: http://ollama:11434

  - model_name: ollama/deepseek-coder
    litellm_params:
      model: ollama/deepseek-coder
      api_base: http://ollama:11434

  - model_name: ollama/qwen2.5-coder
    litellm_params:
      model: ollama/qwen2.5-coder
      api_base: http://ollama:11434

  # ===========================================================================
  # TOGETHER AI - $25 gratuits
  # ===========================================================================
  - model_name: together/llama-3-70b-chat
    litellm_params:
      model: together_ai/meta-llama/Llama-3-70b-chat-hf
      api_key: os.environ/TOGETHER_API_KEY

  - model_name: together/deepseek-coder-33b
    litellm_params:
      model: together_ai/deepseek-ai/deepseek-coder-33b-instruct
      api_key: os.environ/TOGETHER_API_KEY

  # ===========================================================================
  # HUGGING FACE - Gratuit
  # ===========================================================================
  - model_name: hf/qwen-coder
    litellm_params:
      model: huggingface/Qwen/Qwen2.5-Coder-32B-Instruct
      api_key: os.environ/HUGGINGFACE_API_KEY

# =============================================================================
# ROUTER CONFIGURATION - Load balancing et fallback
# =============================================================================
router_settings:
  # Stratégie de routing
  routing_strategy: usage-based-routing  # Distribue selon l'usage
  
  # Retry automatique
  num_retries: 3
  retry_after: 5  # secondes
  
  # Timeout
  timeout: 120  # secondes
  
  # Fallback si provider échoue
  fallbacks:
    - "groq/llama-3.3-70b-versatile": ["ollama/codellama", "ollama/llama3.2"]
    - "groq/llama-3.1-8b-instant": ["ollama/llama3.2"]

# =============================================================================
# CACHE CONFIGURATION - Redis pour performances
# =============================================================================
cache:
  type: redis
  host: redis
  port: 6379
  
  # Cache TTL (time to live)
  ttl: 3600  # 1 heure
  
  # Cache uniquement les réponses réussies
  cache_responses: true
  
  # Paramètres du cache
  cache_params:
    similarity_threshold: 0.8  # Similarité pour hit de cache
    supported_call_types: ["acompletion", "completion", "embedding"]

# =============================================================================
# OBSERVABILITY - Langfuse tracking
# =============================================================================
litellm_settings:
  # Logging
  set_verbose: false
  json_logs: true
  
  # Langfuse integration
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]
  
  # Drop params from logging (privacy)
  drop_params: true
  
  # Rate limiting
  max_parallel_requests: 100
  max_budget: 1000  # USD par mois (pour tracking même si gratuit)
  
  # Guardrails
  guardrails:
    - prompt_injection:
        callbacks: ["langfuse"]
        default_on: true

# =============================================================================
# GENERAL SETTINGS
# =============================================================================
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  
  # Database pour persistence (optionnel)
  # database_url: postgresql://...
  
  # UI
  ui_access_mode: all  # Tous peuvent voir l'UI
  
  # Health check
  health_check_interval: 300  # 5 minutes

# =============================================================================
# NOTES D'UTILISATION
# =============================================================================
# 
# 1. UTILISATION BASIQUE :
#    curl http://localhost:4000/v1/chat/completions \
#      -H "Authorization: Bearer sk-1234" \
#      -H "Content-Type: application/json" \
#      -d '{
#        "model": "groq/llama-3.3-70b-versatile",
#        "messages": [{"role": "user", "content": "Hello!"}]
#      }'
#
# 2. AVEC CACHE :
#    - Première requête : Hit l'API
#    - Requêtes identiques suivantes : Depuis cache Redis (x10 plus rapide)
#
# 3. AVEC FALLBACK :
#    - Si Groq échoue → Automatiquement utilise Ollama
#    - Transparent pour l'utilisateur
#
# 4. LOAD BALANCING :
#    - Distribue les requêtes sur plusieurs providers
#    - Évite les rate limits
#
# 5. UI ADMIN :
#    - http://localhost:4000/ui
#    - Voir les stats, modèles, etc.
#
# =============================================================================
