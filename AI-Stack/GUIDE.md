# üöÄ D√©marrage Rapide - Stack IA Ultime

## En 10 Minutes Chrono ‚è±Ô∏è

### 1Ô∏è‚É£ Obtenir une Cl√© Groq (2 min)

**Pourquoi ?** Groq est gratuit, ultra-rapide (<1s), et excellent qualit√©.

```bash
# 1. Aller sur https://console.groq.com/
# 2. S'inscrire avec email (gratuit)
# 3. Cr√©er une API Key
# 4. Copier la cl√© (commence par gsk_)
```

### 2Ô∏è‚É£ Configurer (2 min)

```bash
cd AI-Ultimate-Stack/

# Cr√©er .env
cp .env.example .env

# √âditer
nano .env
```

**Minimum requis** :
```bash
# Cl√© Groq (OBLIGATOIRE)
GROQ_API_KEY=gsk_votre_cle_ici

# Secrets (g√©n√©rer avec : openssl rand -base64 32)
LITELLM_MASTER_KEY=votre-secret-unique-ici
LANGFUSE_NEXTAUTH_SECRET=secret-32-chars-minimum
LANGFUSE_SALT=autre-secret-32-chars
WEBUI_SECRET_KEY=encore-un-secret-32-chars
LANGFUSE_DB_PASSWORD=mot-de-passe-securise
```

### 3Ô∏è‚É£ D√©marrer (2 min)

```bash
# Lancer tout
docker compose up -d

# Suivre les logs
docker compose logs -f
```

**Attendre** : 2-5 minutes que tout d√©marre et que les mod√®les Ollama se t√©l√©chargent.

### 4Ô∏è‚É£ V√©rifier (1 min)

```bash
# Voir l'√©tat
docker compose ps

# Tous les services doivent √™tre "healthy" ou "running"
```

### 5Ô∏è‚É£ Tester (3 min)

#### Open WebUI (Interface Chat)

```bash
# Ouvrir dans le navigateur
open http://localhost:3000

# 1. Cr√©er un compte (premier utilisateur = admin)
# 2. Aller dans Settings ‚Üí Models
# 3. S√©lectionner "groq/llama-3.3-70b-versatile"
# 4. Commencer √† chatter !
```

#### Goose (Agent Autonome)

```bash
# Lancer une session
docker compose exec goose goose session

# Dans la session Goose, taper :
> Create a Python hello world function

# Goose va le cr√©er automatiquement !
```

#### LiteLLM (API)

```bash
# Test API
curl http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer sk-1234" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "groq/llama-3.3-70b-versatile",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

---

## ‚úÖ C'est Pr√™t !

Vous avez maintenant acc√®s √† :

| Service | URL | Description |
|---------|-----|-------------|
| **Open WebUI** | http://localhost:3000 | Chat type ChatGPT |
| **Langfuse** | http://localhost:3002 | Analytics IA |
| **LiteLLM UI** | http://localhost:4000/ui | Admin gateway |
| **Tabby** | http://localhost:8080 | Autocompl√©tion |
| **Goose** | `docker compose exec goose goose session` | Agent CLI |

---

## üéØ Prochaines √âtapes

### Option 1 : Configurer VSCode

**Tabby (Autocompl√©tion)** :
1. Installer extension "Tabby"
2. Endpoint : `http://localhost:8080`
3. Token : (laisser vide)

**Continue.dev (Chat)** :
1. Installer extension "Continue"
2. Base URL : `http://localhost:4000/v1`
3. API Key : `sk-1234`
4. Model : `groq/llama-3.3-70b-versatile`

### Option 2 : Explorer Langfuse

```bash
# Ouvrir Langfuse
open http://localhost:3002

# 1. Cr√©er un compte
# 2. Cr√©er un projet
# 3. Copier les cl√©s API
# 4. Les mettre dans .env :
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...

# 5. Red√©marrer
docker compose restart litellm open-webui
```

### Option 3 : Activer Perplexica (Recherche Web)

```bash
# D√©marrer avec Perplexica
docker compose --profile perplexica up -d

# Acc√©der
open http://localhost:3001
```

---

## üêõ Probl√®mes Courants

### "No space left on device"

```bash
# Les mod√®les Ollama prennent de la place
# R√©duire les mod√®les dans .env :
OLLAMA_MODELS=llama3.2

# Ou augmenter l'espace Docker
```

### "Out of memory"

```bash
# Utiliser des mod√®les plus petits
TABBY_MODEL=StarCoder-1B
OLLAMA_MODELS=llama3.2
OLLAMA_NUM_CTX=2048
```

### Groq API Key invalide

```bash
# V√©rifier dans .env
echo $GROQ_API_KEY

# Re-cr√©er une cl√© sur https://console.groq.com/
```

### Services pas healthy

```bash
# Voir les logs
docker compose logs <service>

# Red√©marrer
docker compose restart <service>
```

---

## üí° Conseils

### Cache Redis (Vitesse x10)

Le cache Redis est activ√© automatiquement. Les requ√™tes identiques sont **100x plus rapides** !

```
Requ√™te 1 : "Explain async" ‚Üí 2s (API)
Requ√™te 2 : "Explain async" ‚Üí 0.02s (cache) ‚ö°
```

### Load Balancing

Si Groq est rate-limited, LiteLLM bascule automatiquement sur un autre provider :

```
Groq ‚Üí Together AI ‚Üí Ollama
```

### Analytics

Langfuse track **toutes** les requ√™tes :
- Latence
- Co√ªts
- Erreurs
- Conversations compl√®tes

Parfait pour d√©bugger !

---

## üéâ F√©licitations !

Vous avez une stack IA **ultra-compl√®te** et **optimis√©e** :

- ‚úÖ Cache Redis (x10-100 vitesse)
- ‚úÖ Load balancing automatique
- ‚úÖ Fallback multi-providers
- ‚úÖ Analytics temps r√©el
- ‚úÖ Agent autonome (Goose)
- ‚úÖ Interface moderne
- ‚úÖ 100% gratuit et auto-h√©berg√©

**Consultez le README.md complet pour aller plus loin ! üöÄ**
